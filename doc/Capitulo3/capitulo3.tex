%?????????????????????????
% Nombre: capitulo3.tex  
% 
% Texto del capitulo 3
%---------------------------------------------------

\chapter{Clasificación}
\label{clasificacion}

Como hemos visto antes es posible ir añadiendo variables a nuestras tablas y obtener reglas al respecto, pero esto se complica a medida que nuevas variables son añadidas de manera que se hace inmanejable. Algunos algoritmos de aprendizaje como los arboles de decisión realizan esta ardua labor por nosotros por lo que podemos apoyarnos en ellos para obtener mejores resultados y aumentar aún más el accuracy en nuestro clasificador. Esta tarea es la que aborda este capítulo. 


\section{Clasificadores en Knime}

Antes de ponernos a utilizar clasificadores e instalar paquetes sin criterio alguno en R, cabe quizá dedicar unos minutos a hacer una comparativa de clasificadores para después centrarnos en el uso de uno de estos en R de manera que podamos obtener toda la potencia de esta herramienta. Para este menester, ya que hemos usado Knime en la anterior sección, usaremos algunos de los algoritmos que Knime recoge para clasificar el problema, tras ello, haremos una comparativa de los resultados y los que nos ofrezcan mejor resultados serán los que trasladaremos a R. 

En la figura \ref{clasificadores} podemos encontrar el flujo de Knime usado. 

\begin{figure}[h]
	\centering
		\includegraphics[scale=0.7]{./Capitulo3/imagenes/clasificadores.png}
		\caption{Distintos algoritmos de clasificación usados.}
	\label{clasificadores}
\end{figure} 

Los clasificadores usados han sido los siguientes:

\begin{itemize}

	\item \textbf{\textit{Naïve Bayes}} \cite{naive}: Es el algoritmo más sencillo tratado dado su carácter probabilístico. Como su propio nombre indica, está basado en el teorema de Bayes y su comportamiento es bastante aceptable cuando trabajamos con datos discretos. Su fundamento está en asumir la independencia de cada una de las características tratadas, obteniendo la probabilidad de pertenencia a cada clase en base a cada una de las características por separado. Estas a su vez serán combinadas obteniendo el resultado final. 
	
	\item \textbf{\textit{MLP-BP}} \cite{mlp}: Este algoritmo está basado en redes neuronales artificiales y su fundamento está en la utilización de neuronas (objeto lógico) con diversas entradas a partir de las cuales se realizan sumas ponderadas que unidas a algún tipo de función producen una salida que puede volver a ser la entrada de otra neurona formando así nuestra red neuronal. MLP consta de varias capas una de entrada que tiene tantas neuronas como variables a tener en cuenta en la clasificación, una oculta que realiza el procesamiento y una capa de salida con una neurona por cada una de las clases posibles a tener en cuenta. Para poder usar este algoritmo se ha realizado un preprocesado con el cual pasamos a variables continuas las categóricas, el flujo para llevar a cabo este preprocesado podemos verlo en la figura \ref{nn}.
		
	
	\item \textbf{\textit{SMO}} \cite{smo}: Este algoritmo está basado en \textit{Support Vector Machines}. Los algoritmos de este grupo, son algunos de los más potentes en el campo del aprendizaje supervisado y consisten en construir hiperplanos en un espacio de dimensionalidad muy alta de manera que al haber llevado los datos a estas dimensiones tan grandes la separación entre ellos permita realizar una buena clasificación de una nueva instancia que se agrupará a una de las clases. 
		
	\item \textbf{\textit{C4.5}} \cite{c45}: Es una extensión del algoritmo ID3 \cite{id3} y está basado en árboles de decisión. Estos árboles se construyen en base a unos datos de entrenamiento usando el concepto de entropía de la información. En base a este concepto, se van eligiendo atributos de los disponibles en los datos de entrenamiento para ir dividiendo en cada nodo del árbol las muestras a clasificar, de este modo en las hojas del árbol tendremos las muestras finales clasificadas.
	
	\item \textbf{\textit{RandomForest}}: Este algoritmo es un ensemble de Arboles de decisión. Los ensembles funcionan por votaciones, es decir, se generan arboles de decisión para distintas variables y ante un ejemplo concreto si hay tres arboles que lo barajan y 2 dicen que sobrevive se tomará  como que este ejemplo sobrevive. Los RandomForest tienen una ventaja y es que evitan el problema del sobre-entrenamiento. Esto lo consiguen añadiendo aleatoriedad al conjunto dos formas distintas: mediante el bagging lo que generamos es que los arboles se generan con conjuntos de muestras aleatorias en cada vez. Podemos estimar que se dejan fuera un 37\% de las muestras  y que algunas serán incluso repetidas. Por otro lado, el segundo factor para añadir aleatoriedad es la selección de variables, mediante la cual los arboles generados van cambiando las variables que toman para predecir.
	
\end{itemize}

\begin{figure}[h]
	\centering
		\includegraphics[scale=0.7]{./Capitulo3/imagenes/nn.png}
		\caption{Preprocesado para redes neuronales.}
	\label{nn}
\end{figure} 

Potro otro lado, para prevenir el posible problema del sobre-entrenamiento hemos llevado a cabo para cada uno de los clasificadores una validación cruzada con k=10 submuestras, este flujo es facilitado por Knime en el metanodo \textit{\textbf{CrossValidation}}, cuyo contenido podemos verlo en la figura \ref{cross}. Una vez creado nuestro flujo en Knime y ejecutarlo obtenemos los resultados de los clasificadores los cuales podemos encontrarlos en la tabla \ref{tabla_resultados}.  

\begin{figure}[h]
	\centering
		\includegraphics[scale=0.7]{./Capitulo3/imagenes/cross.png}
		\caption{Detalle del nodo de cross validation.}
	\label{cross}
\end{figure} 

\begin{table}[]
\centering
\label{tabla_resultados}
\begin{tabular}{cc}
{\ul \textbf{Algoritmo}}               & {\ul \textbf{Accuracy}} \\
C4.5 (Decision Tree)                   & 0.79807                 \\
Näive Bayes (Probabilistic)            & 0,80022                 \\
MLP (Neural Network)                   & 0,82493                 \\
SMO (Support Vector Machine)           & 0,70146                 \\
Random Forest (Decision Tree Ensemble) & 0,8353                 
\end{tabular}
\caption{Resultados de la clasificación para distintos algoritmos}
\end{table}

Podemos comprobar que parece ser que un RandomForest, será nuestra mejor opción, pero si nos fijamos en la figura \ref{fig_survived}, encontramos de nuevo que las clases están bastante desequilibradas, lo que puede hacer que nuestro sistema se comporte de manera errónea a la hora de evaluar. Retomemos el ejemplo del modelo 1 (sección \ref{modelo1}), es decir, no realizar ninguna clasificación y asignar la clase mayoritaria a todos los ejemplos. En nuestro problema ya causaba el que tuviéramos un \textbf{Accuracy de 0.60} cuando en realidad no estábamos clasificando nada, imaginar por un momento que pasaría si aplicáramos este sistema clasificador a un sistema experto que predice la existencia o no de  una enfermedad en la que tenemos 99 personas sanas por cada persona enferma? ¿0.99 de Accuracy? Nada más lejos de la realidad, hay casos en los que esto sucede y en los que deberemos ponderar un fallo en la clase negativa por encima de los aciertos en la clase positiva o viceversa en función de lo que estemos intentando clasificar. 

Teniendo en tanto por cuenta que en nuestro problema tenemos una clase en clara minoría y con el fin de intentar afinar en el proceso de elección de nuestro clasificador utilizaremos un método de evaluación alternativo como es el caso de la \textit{curva ROC}, método interesante y que nos ofrecerá resultados más fiables que evitan el sesgo hacia la clase mayoritaria que se dan en otros métodos. Las curvas ROC y su correspondientes áreas bajo la curva de los clasificadores usados podemos verlas en las figuras \ref{roc1}, \ref{roc2}, \ref{roc3}, \ref{roc4}.


\begin{figure}
	\centering
		\includegraphics[scale=0.3]{./Capitulo3/imagenes/roc1.png}
		\caption{Curva ROC para árboles de decisión.}
	\label{roc1}
\end{figure} 

\begin{figure}
	\centering
		\includegraphics[scale=0.3]{./Capitulo3/imagenes/roc2.png}
		\caption{Curva ROC para Näive Bayes.}
	\label{roc2}
\end{figure} 

\begin{figure}
	\centering
		\includegraphics[scale=0.3]{./Capitulo3/imagenes/roc3.png}
		\caption{Curva ROC para SMO.}
	\label{roc3}
\end{figure} 

\begin{figure}
	\centering
		\includegraphics[scale=0.3]{./Capitulo3/imagenes/roc4.png}
		\caption{Curva ROC para Random Forest.}
	\label{roc4}
\end{figure} 


Dado los resultados de los valores de Accuracy y tras constatar que estos se ajustan a la realidad con el análisis de la curva ROC y su área bajo la curva  nos centraremos en \textbf{RandomForest} de cara a las predicciones para la competición. Pasaremos de nuevo a R ya que nos ofrece más facilidad para crear el fichero tipo que Kaggle acepta en las competiciones y nos ofrece la posibilidad de evaluar fácilmente sobre el conjunto de Test que debemos subir a Kaggle para su evaluación.

\section{RandomForest}
\label{forest}

Para poder seguir subiendo posiciones en la competición nos centraremos por tanto en el uso de \textit{\textbf{RandomForest}} en Rstudio. El script que usaremos podemos encontrarlo en las siguientes líneas, sobre este mismo script se han realizado a modo de comentarios las explicaciones del código para facilitar al lector la comprensión de cada uno de los pasos y decisiones tomadas. 

\begin{lstlisting}
          
          "Vamos a trabajar con RandomForest. Este es un ensemble de arboles de
         decisión. Los ensembles funcionan por votaciones, es decir, se generan 
         arboles de decisión para distintas variables y ante un ejemplo concreto si
         hay tres arboles que lo barajan y 2 dicen que sobrevive se tomará como
         que este ejemplo sobrevive. Los RandomForest tienen una ventaja y es que
         evitan el problema del sobreentrenamiento. Esto lo consiguen añadiendo
         aleatoriedad al conjunto de la siguiente manera:
    
      -Bagging: Mediante el bagging lo que generamos es que los arboles se generan
    con conjuntos de muestras aleatorias en cada vez. Podemos estimar que se dejan
    fuera un 37% de las muestras y que algunas serán incluso repetidas.
    
      -Selección de variables: Los árboles generados van cambiando las variables
    que toman para predecir."
    
    
    "Para afinar las variables que usaremos en el proceso de entrenamiento del 
    algoritmo podemos usar las siguientes sentencias que nos ofrecerán información
    de como se reduce el error según el facto mTry"
    
    mtryTunning <- function(data) 
    {
  		set.seed(345)
 		 suppressMessages(mtry <- tuneRF(data[, names(data) %in% attr(terms(fml), "term.labels")],
                                  data[, names(data) %in% c("Survived")],
                                  stepFactor=3.0,
                                  improve=1e-8,
                                  ntree=500,
                                  trace=FALSE))
  		return(mtry)
	}

	fml <-Survived ~ Sex + Age + Fare + Pclass + SibSp + Parch + FamilySize + Embarked + TitleWO
	print(mtryTunning(train))
    
    	"Podemos ver como este valor estará entre 1 y 3"
    
  	"Comenzamos con el RandomForest de R"
      
    	fit <- randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare +
                    Embarked,
                    data=train,
                    importance=TRUE,
                    ntree=2000)

	varImpPlot(fit)

	Prediction <- predict(fit, test)    
    	submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
    
    	write.csv(submit, file = "output.csv", row.names = FALSE)
    
    #MODELO CON ACCURACY 0.78469

    "********************************************************************************"
    
   	 "Parece que funciona bien pero ahora añadiremos nuevos factores como las variables 
    generadas anteriormente en nuestro preprocesado."
    
    	fit <- randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare +
                    Embarked+IsChild+IsMother+Title,
                    data=train,
                    importance=TRUE,
                    ntree=2000)

	varImpPlot(fit)
    
    	"Los resultados son similares al punto anterior, aunque title es la mejor variable tal y como 
    vemos en el gráfico, las demás que hemos introducido quizá introducen ruido  y no aportan nada. "
    
    "********************************************************************************"
    
    	"Vamos a usar los paquetes de la libreria rparty para nuestro RandomForest y solo usaremos
    la variable Title de las nuevas."
    
    	set.seed(345)
		fit <- cforest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare +
                 	Embarked + Title,
               		data = train, 
               		controls=cforest_unbiased(ntree=2000, mtry=3))
		
	#MODELO CON ACCURACY 0.79904	
	
	Prediction <- predict(fit, test, OOB=TRUE, type = "response")
	submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
	
     "********************************************************************************"	
    	 "Ahora vamos a comprobar que tal se comporta el modelo con nuestras variables sin outliers."
	
	set.seed(345)
		fit <- cforest(as.factor(Survived) ~ Pclass + Sex + AgeWO + SibSp + Parch + FareWO +
                 	Embarked + Title,
               		data = train, 
               		controls=cforest_unbiased(ntree=2000, mtry=3))
		
	#MODELO CON ACCURACY 0.80383	
	
	Prediction <- predict(fit, test, OOB=TRUE, type = "response")
	submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
    
    "*********************************************************************************"
    
    fit <- cforest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare +
               Embarked + Title + FamilySize + FamilyID,
               data = train, 
               controls=cforest_unbiased(ntree=2000, mtry=3))

	Prediction <- predict(fit, test, OOB=TRUE, type = "response")

	submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
	
	#MODELO CON ACCURACY 0.8134
    
    \end{lstlisting}


Algunas salidas interesantes del anterior código podría ser la que podemos ver en la figura \ref{split}, que corresponde con la función \textbf{\textit{varImpPlot(fit)}} en ella podemos comprobar como las variables de nuestro árbol de decisión se comportan y hacen aumentar el Accuracy de este a medida que este avanza su entrenamiento. Esto es útil para ver que variables son realmente importantes en nuestro proceso de entrenamiento y en ella hemos podido comprobar como a pesar de lo que pensábamos en puntos anteriores las variables \textbf{IsMother} e \textbf{IsChild}, no ofrecerán mucho de cara a entrenar el modelo si lo comparamos con la variable \textbf{Title} por ejemplo que es la que más afecta. 

\begin{figure}[H]
	\centering
		\includegraphics[scale=0.5]{./Capitulo3/imagenes/split.png}
		\caption{Comportamiento variables en el RandomForest.}
	\label{split}
\end{figure} 

Por otro lado, un factor importante en RandomForest, es discernir entre el valor de mTry, es decir, las variables que serán tomadas en cuenta en el proceso iterativo de generación de árboles con aleatoriedad, por ello en las primeras líneas del script anterior realizamos un entrenamiento para ver como aumenta el error en función del valor que tome esta variable, el resultado de este, lo encontramos en la figura \ref{errormtry}. En ella podemos ver como el valor optimo estará entre 1 y 3.

\begin{figure}[H]
	\centering
		\includegraphics[scale=0.5]{./Capitulo3/imagenes/error.png}
		\caption{Obtencion del valor mtry.}
	\label{errormtry}
\end{figure} 
 

De la manera que hemos visto en esta sección, podremos construir modelos muy útiles, de hecho, hemos llegado a obtener el valor de 0.8134, pero aun estos valores se pueden mejorar. Para ello, construiremos modelos avanzados y con parámetros muy afinados del algoritmo RandomForest, siguiendo el tutorial de  Marcio Gualtieri \cite{marcio}. 

\section{HyperTunned Random Forest}

En esta sección veremos un modelo avanzado de RandomForest y obtendremos el resultado final de la competición que está en 0.82297 de accuracy en test y posición 218. Explicaremos el script seguido como es ya habitual en los comentarios del mismo. 

\begin{lstlisting}

	"Creamos constructores para cada uno de los métodos que usaremos."

	RandomForestBuilder <- function() 
	{
  		mtry = 1
  
  		name = paste("Random Forest (randomForest) mtry:", mtry)
  
  		model = function(fml, data) {
    		
		set.seed(345)
    		numVars <- length(attr(terms(fml), "term.labels"))
    		maxMtry <- floor(sqrt(numVars))
    
    		if (mtry > maxMtry) {
      			return(randomForest(formula(fml), data = data))
    		}
    		
		return(randomForest(formula(fml), data = data, mtry = mtry))
 	 	}
  
  		predictions = function(model, data) 
		{
    			return(stats::predict(model, newdata = data, type = "class"))
  		}
  
  		probabilities = function(model, data) 
		{
   		 	result <- predict(model, newdata=data, type="prob")
   		 	return(result[, 2])
 	 	}
  
  		return(list(name=name, model=model, predictions=predictions, probabilities=probabilities))
	}

	RandomForestRandomCVBuilder <- function() {
  		name = "Random Forest with Random Search Cross Validation (rpart)"
  
  		model = function(fml, data) 
		{
    			set.seed(345)
    			return(randomForestRandomCrossValidation(fml, data, "Accuracy", radius=10, repeats=3))
  		}
  
  		predictions = function(model, data) {
    		return(stats::predict(model, newdata = data, type = "raw"))
  	}
  
  	probabilities = function(model, data) 
	{
    		result <- predict(model, newdata=data, type="prob")
    		return(result[, 2])
  	}
  
  	return(list(name=name, model=model, predictions=predictions, probabilities=probabilities))
	}

	DecisionTreeBuilder <- function()
	 {
  		name = "Decision Tree (rpart)"
  
  			model = function(fml, data) {
    				set.seed(345)
    				return(rpart(fml, data = data, method="class"))
  			}
  
  		predictions = function(model, data) {
    			return(stats::predict(model, newdata = data, type = "class"))
  		}
  
 		 probabilities = function(model, data) {
    			result <- predict(model, newdata=data, type="prob")
    			return(result[, 2])
  		}
  
  	return(list(name=name, model=model, predictions=predictions, probabilities=probabilities))
	}

	ConditionalInferenceBuilder <- function() 
	{
  		mtry = 1
  
  		name = paste("Conditional Inference Forest (party) mtry: ", mtry)
  
  		model = function(fml, data) {
    			set.seed(345)
    
    			numVars <- length(attr(terms(fml), "term.labels"))
    			maxMtry <- floor(sqrt(numVars))
    
   		 	if (mtry > maxMtry) {
      				return(cforest(fml, data = data, controls=cforest_unbiased()))
    			}
    			return(cforest(fml, data = data, controls=cforest_unbiased(mtry = mtry)))
  		}
  
  		predictions = function(model, data) {
   			 return(stats::predict(model, newdata = data, type = "response"))
  		}
  
  		probabilities = function(model, data) {
    			result <- predict(model, newdata=data, type="prob")
    			result <- lapply(result, `[`, 2)
    			return(result)
  		}
  
  	return(list(name=name, model=model, predictions=predictions, probabilities=probabilities))
	}


	"Creamos funciones para la evaluación en training de los clasificadores"


	fmeasure <- function(confusion) 
	{
  		p <- confusion$byClass["Pos Pred Value"]
  		r <- confusion$byClass["Sensitivity"]
  		f <- 2 * ((p * r) / (p + r))
  		return(f)
	}

	accuracy <- function(confusion) 
	{
  		return(confusion$overall["Accuracy"])
	}

	createFormulaName <- function(fml)
	 {
  		formulaName <- paste(format(fml), collapse = "")
  		formulaName <- str_replace_all(formulaName, "[\\s]", "")
  		return(formulaName)
	}

	evaluateModel <- function(fml, ModelBuilder, predictionColumn, trainData, testData)
	 {
  		formula <- createFormulaName(fml)
  		modelBuilder <- ModelBuilder()
  		model <- modelBuilder$model(fml, trainData)
  		predictions <- modelBuilder$predictions(model, testData)
  		confusion <- confusionMatrix(data = predictions, reference = testData$Survived)
  		evaluation <- data.frame(model = modelBuilder$name, 
                           formula = formula,
                           accuracy = accuracy(confusion), 
                           fmeasure = fmeasure(confusion), 
                           stringsAsFactors=FALSE)
 		 return(evaluation)
	}


	evaluateModels <- function(formulas, ModelBuilders, predictionColumn, data, testData = NULL) {
  		set.seed(3456)
  		if(is.null(testData)) 
		{
    			trainIdx <- createDataPartition(data$Survived, p = 0.6, list = FALSE, times = 1)
    			trainData <- data[trainIdx,]
    			testData <- data[-trainIdx,]
  		} else {
    			trainData <- data
  		}
  
  		evaluations <- data.frame(model = character(), 
                            formula = character(), 
                            accuracy = numeric(0), 
                            fmeasure = numeric(0), 
                            stringsAsFactors=FALSE)
  
  		for (ModelBuilder in ModelBuilders) {
    			for (fml in formulas) {
      				evaluation <- evaluateModel(fml, ModelBuilder, predictionColumn, trainData, testData)
      				evaluations <- bind_rows(evaluations, evaluation)
    			}
  		}
  	renderTable(evaluations)
	}


	"Hacemos particiones del conjunto de train para entrenar con el. Nos quedaremos 
	con el 60% para entrenar y el 40% restante para comprobar que tal va nuestro modelo.
	Podríamos usar el test de Kaggle, pero para evaluar este y saber como funciona en 
	nuestro modelo deberíamos subir los resultados a kaggle lo que no sería apropiado. 
	Por lo tanto entrenaremos nuestro modelo con este conjunto de test proveniente del 
	training y cuando lo tengamos ajustado y veamos cual ofrece un buen resultado 
	aplicaremos el modelo al test real y evaluaremos en kaggle. "


	trainIdx <- createDataPartition(train$Survived, p = 0.6, list = FALSE, times = 1)
	trainData <- train[trainIdx,]
	testData <- train[-trainIdx,]


	trainData <- addSurvivalRate("Surname", trainData, trainData)
	testData <- addSurvivalRate("Surname", testData, trainData)


	trainData <- addSurvivalRate("Ticket", trainData, trainData)
	testData <- addSurvivalRate("Ticket", testData, trainData)

	trainData <- addSurvivalRate("FamilyID", trainData, trainData)
	testData <- addSurvivalRate("FamilyID", testData, trainData)

	trainData <- addSurvivalRate("FamilyIDWO", trainData, trainData)
	testData <- addSurvivalRate("FamilyIDWO", testData, trainData)


	"Creamos distintas fórmulas de combinaciones para ver cuales son las que 
	mejor se comportan a la hora de entrenar el modelo."

	formulas <- c(Survived ~ Sex,
              Survived ~ Sex + Age,
              Survived ~ Sex + Age + Fare,
              Survived ~ Sex + Age + Fare + Pclass,
              Survived ~ Sex + Age + Fare + Pclass + SibSp,
              Survived ~ Sex + Age + Fare + Pclass + SibSp + Parch,
              Survived ~ Sex + Age + Fare + Pclass + FamilySize,
              Survived ~ Sex + Age + Fare + Pclass + FamilySize + Embarked,
              Survived ~ Sex + Age + Fare + Pclass + FamilySize + Embarked + Title,
              Survived ~ Sex + Age + Fare + Pclass + FamilySize + Embarked + TitleWO,
              Survived ~ Sex + AgeWO + Fare + Pclass + FamilySize + Embarked + TitleWO,
              Survived ~ Sex + AgeWO + FareWO + Pclass + FamilySize + Embarked + TitleWO,
              Survived ~ Sex + AgeWO + FareWO + Pclass + FamilySize + Embarked + TitleWO + IsChild,
              Survived ~ Sex + AgeWO + FareWO + Pclass + FamilySize + Embarked + TitleWO + IsChild + IsMother,
              Survived ~ Sex + AgeWO + FareWO + Pclass + FamilySize + Embarked + TitleWO + IsChild + IsMother + IsAlone)


	"Añadimos aun array los modelos que serán comparados."
	
	models <- c(RandomForestBuilder, ConditionalInferenceBuilder)

	"Evaluamos los modelos"
	
	evaluateModels(formulas, models, "Survived", trainData, testData)


	"Evaluación basada en curvas ROC"

	createPrediction <- function(fml, ModelBuilder, testData) {
  		modelBuilder <- ModelBuilder()
  		model <- modelBuilder$model(fml, trainData)
  		probabilities <- modelBuilder$probabilities(model, testData)
  		return(prediction(as.numeric(probabilities), testData$Survived))
	}

	createPerformance <- function(prediction) {
  		return(performance(prediction, measure = "tpr", x.measure = "fpr"))
	}

	createAuc <- function(prediction) {
  		auc <- performance(prediction, measure = "auc")
  		return(auc@y.values[[1]])
	}

	buildModelName <- function(fml, ModelBuilder, auc) {
  		formulaName <- createFormulaName(fml)
  		aucName <- paste("(AUC: ", auc, ")")
  		modelBuilder <- ModelBuilder()
  		name <- paste0(modelBuilder$name, "\n", formulaName, "\n", aucName, "\n")
  		return(name)
	}

	createRocPlot <- function(roc) {
  		return(list(geom_ribbon(data = roc, aes(x=FPR, fill = Model, ymin=0, ymax=TPR), alpha = 0.2),
              geom_line(data = roc, aes(x=FPR, y=TPR, color = Model))))
	}

	generateAllROCs <- function(formulas, ModelBuilders, trainData, testData) {
  		rocPlots <- c()
  		for (ModelBuilder in ModelBuilders) {
    			for (fml in formulas) {
      				modelBuilder <- ModelBuilder()
      				prediction <- createPrediction(fml, ModelBuilder, testData)
      				performance <- createPerformance(prediction)
      				auc <- createAuc(prediction)
      				roc <- data.frame(FPR=unlist(performance@x.values),
                        		TPR=unlist(performance@y.values),
                       		 Model=rep(buildModelName(fml, ModelBuilder, auc), each=length(performance@x.values)))
     				 rocPlots <- c(rocPlots, createRocPlot(roc))
    			}
  		}
  
  		roc <- data.frame(FPR=c(0.0, 1.0),
                 TPR=c(0.0, 1.0),
                  Model=rep("Line of No-discrimination\n", each=2))
  		rocPlots <- c(rocPlots, createRocPlot(roc))
  		ggplot() + rocPlots + coord_fixed()
	}


	"Dado que estamos ante un problema con una clase en clara minoría, quizá basarnos
	solo en el Acc sea muy optimista a la hora de evaluar por ello, nos basaremos también
	en la curva ROC evitando el sesgo hacia la clase mayoritaria."

	generateAllROCs(formulas, models, trainData, testData)

\end{lstlisting}

Merece la pena pararse a explicar ciertos puntos del anterior código. En primera instancia, estamos generando constructores para los distintos clasificadores que usaremos, RandomForest normal y ConditionalInference concretamente. También implementaremos un método que implementa cross validation para evitar los problemas de \textit{overfitting}. 

Una vez dividido el conjunto de training en dos, para poder evaluar sobre este el modelo, definimos en la variable fórmulas cada una de las posibles combinaciones de variables en nuestro problema. También, seleccionamos los modelos a construir y los añadimos al array \textbf{modelos}. Una vez hecho esto, podremos ejecutar, tras lo cual, nuestra función renderiza en HTML la tabla que podemos ver en la figura \ref{fig_resultados}.

\begin{figure}[H]
	\centering
		\includegraphics[scale=0.4]{./Capitulo3/imagenes/resultados.png}
		\caption{Resultados de la evaluación.}
	\label{fig_resultados}
\end{figure} 

Si nos fijamos en ella, los resultados obtenidos por el modelo basado en RandomForest normal ofrece resultados significativamente superiores al del constructor basado en \textit{conditional inference}. Por otro lado, las formulas que parecen comportarse mejor son las dos más completas, barajando también las variables básicas sin outliers. 

Por otro lado, a modo de experimento se han comprobado también los resultados con un valor de mTry distinto de 1, y los resultados usando los valores de 2 y 3 para este parámetro son casi iguales aunque ligeramente peor, lo que corrobora lo visto en la figura \ref{errormtry}.


Para finalizar esta sección, como hemos explicado antes, al tener una clase en minoría, puede ser que tengamos cierto problema de overfitting, por ello, para evaluar verazmente en casos como estos, podemos usar el \textbf{AUC}, o área bajo la curva ROC.  En el anterior script también se implementan los métodos para ello y los resultados podemos verlos en la siguientes imágenes. 

\begin{figure}[H]
	\centering
		\includegraphics[scale=0.4]{./Capitulo3/imagenes/cf.png}
		\caption{Curvas Roc para conditional inference.}
	\label{cf}
\end{figure} 

\begin{figure}[H]
	\centering
		\includegraphics[scale=0.4]{./Capitulo3/imagenes/rf.png}
		\caption{Curvas Roc para RandomForest.}
	\label{rf}
\end{figure} 


Respecto a las curvas Roc, los resultados son ligeramente superiores que la evaluación basada en Accuracy, pero tampoco nos dan mucha más información que la que ya tendríamos, por lo que en base a nuestros experimentos, la formula ganadora será:

\textit{\textbf{RandomForest + Survived~Sex+AgeWO+ FareWO
 +Pclass+ FamilySize+Embarked +TitleWO+
 IsChild+IsMother+IsAlone}} 

Si entrenamos el modelo con este paso y evitamos el overfitting, usando el constructor de \textit{cross validation}, llegaremos a la puntuación de 0.82 en test. El código final de evaluación, se ha omitido de la memoria ya que no es relevante de cara al estudio pero puede encontrarse tanto en gitHub \cite{github} como en el script en Kaggle \cite{kernel}, el cual, puede ejecutarse en la propia plataforma de manera muy sencilla. 


\section{XGBoost}
\label{boosting}

Pese a que los resultados obtenidos en la sección anterior son buenos, hemos realizado un estudio del problema usando el algoritmo XGBoost en cual se sitúa en el estado del arte dentro de los problemas de clasificación actuales y  es propicio a obtener grandes resultados. El paquete usado es \textbf{xgboost} y el pequeño script podemos verlo a continuación. 

\begin{lstlisting}

	labels<- trainOriginal[,2]
	labels <- as.numeric(labels)

	"Para que Xgboost funcione debemos dummificar las variables."
	
	feature_selection <- function(data) 
	{
  		selected <- c('SibSp', 'Parch', 'Sex', 'Age', 'Fare', 'Pclass', 'Embarked')
  		newdata <- data[,selected]
  		newdata$SibSp <- as.numeric(newdata$SibSp)
  		newdata$Parch <- as.numeric(newdata$Parch)
  		newdata$Age <- as.numeric(newdata$Age)
  		newdata$Age[is.na(newdata$Age)] <- 0
  		newdata$Sex <- as.character(newdata$Sex)
  		newdata$Sex[newdata$Sex == 'male'] <- 1
  		newdata$Sex[newdata$Sex == 'female'] <- 0
  		newdata$Sex <- as.numeric(newdata$Sex)
  		newdata$Fare <- as.numeric(newdata$Fare)
  		newdata$Fare[is.na(newdata$Fare)] <- 0
  		newdata$Pclass <- as.numeric(newdata$Pclass)
  		newdata$Embarked <- as.character(newdata$Embarked)
  		newdata$Embarked[(newdata$Embarked == 'S')] = 0
  		newdata$Embarked[(newdata$Embarked == '')] = 0
  		newdata$Embarked[(newdata$Embarked == 'C')] = 1
  		newdata$Embarked[(newdata$Embarked == 'Q')] = 2
  		newdata$Embarked <- as.numeric(as.character(newdata$Embarked))
  		newdata <- as.matrix(newdata)
  		return(newdata)
	}

	ml <- xgboost(data = feature_selection(trainOriginal), label = labels, 
              nfold = 5, nrounds = 10, objective = "binary:logistic")

	pred <- predict(ml, feature_selection(testOriginal))
	pred[pred > .7] <- 1
	pred[pred <= .7] <- 0

	# Output in csv for submission
	submission <- data.frame(PassengerId = testOriginal$PassengerId, Survived = pred)
	
	#Con probabilidad 0.7 ->acc= 0.77512

\end{lstlisting}

El script anterior es sencillo, pero los resultados que ofrece no son los mejores ya que por un lado las variables que usa xgboost, deben ser \textit{dummificadas} y el proceso que usamos para ello, quizá no sea el mejor. 

Por otro lado, cabe destacar como aumentando la probabilidad de morir, es decir, no dejando en 0.5 el discernir cuando un ejemplo morirá o vivirá sino haciendo mas probable la muerte, ofrece mejores resultados. 

Por último, destacar que este algoritmo es muy sensible al ruido entre variables, es decir, clases etiquetadas de manera distinta con variables muy parecidas, problema muy acentuado en este dataset debido a que al ser datos de un desastre naval real, el sobrevivir o no, aparte de unas pequeñas reglas como las mujeres o los niños, no seguía un orden especifico sino que por duro que parezca era fruto del azar, por lo que este ruido está presente lo que hace que el XGBoost no sea el mejor algoritmo para este problema. 

\pagebreak
\clearpage
%---------------------------------------------------